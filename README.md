# deep-learning-topics
Deep Learning Topics 

What are Discriminative Models, give examples?

What is the difference between generative and discriminative models?

What are Autoencoders and How Do They Work?

What are some popular autoencoders, mention few?

What is the role of the Loss function in Autoencoders, & how is it different from other NN?

How do autoencoders differ from (PCA)?

Which one is better for reconstruction linear autoencoder or PCA?

How can you recreate PCA with neural networks?

Can You Explain How Autoencoders Can be Used for Anomaly Detection?

What are some applications of AutoEncoders

How can uncertainty be introduced into Autoencoders, & what are the benefits and challenges of doing so?

Can you explain what VAE is and describe its training process?

Explain what Kullback-Leibler (KL) divergence is & why does it matter in VAEs?

Can you explain what reconstruction loss is & it’s function in VAEs?

What is ELBO & What is this trade-off between reconstructionQuality & regularization?

Can you explain the training & optimization process of VAEs?

How would you balance reconstructionQuality and latent space regularization in a practical Variational Autoencoder implementation?

What is Reparametrization trick and why is it important?

What is DGG "Deep Clustering via a Gaussian-mixture Variational Autoencoder (VAE)” with Graph Embedding

How does a neural network with one layer and one input and output compare to a logistic regression?

In a logistic regression model, will all the gradient descent algorithms lead to the same model if run for a long time?

What is padding and why it’s used in Convolutional Neural Networks (CNNs)?

Padded Convolutions: What are Valid and Same Paddings?

What is stride in CNN and why is it used?

What is the impact of Stride size on CNNs?

What is Pooling, what is the intuition behind it and why is it used in CNNs?

What are common types of pooling in CNN?

Why min pooling is not used?

What is translation invariance and why is it important?

How does a 1D Convolutional Neural Network (CNN) work?

What are Recurrent Neural Networks, and walk me through the architecture of RNNs.

What are the main disadvantages of RNNs, especially in Machine Translation Tasks?

What are some applications of RNN?

What technique is commonly used in RNNs to combat the Vanishing Gradient Problem?

What are LSTMs and their key components?

What limitations of RNN that LSTMs do and don’t address and how?

What is a gated recurrent unit (GRU) and how is it different from LSTMs?

Describe how Generative Adversarial Networks (GANs) work and the roles of the generator and discriminator in learning.

What are token embeddings and what is their function?

What is Multi-Head Self-Attention and how does it enable more effective processing of sequences in Transformers?

What are transformers and why are they important in combating problems of models like RNN and LSTMs?

Walk me through the architecture of transformers.

What are positional encodings and how are they calculated?

Why do we add positional encodings to Transformers but not to RNN or LSTMs?
